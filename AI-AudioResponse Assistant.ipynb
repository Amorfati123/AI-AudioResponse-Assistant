{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee01b19a",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\"AI-AudioResponse Assistant\" is an advanced virtual assistant built in Python, integrating cutting-edge artificial intelligence with sophisticated audio processing. This project is a showcase of utilizing diverse AI models for natural language understanding and generation, combined with real-time audio interaction capabilities.\n",
    "\n",
    "# Technical Overview\n",
    "The project utilizes a blend of technologies and AI models:\n",
    "\n",
    "## Python: The primary programming language for development.\n",
    "## SoundDevice and PyAudio: For audio recording and playback operations.\n",
    "## Wavio: Handling WAV file operations.\n",
    "## Pygame: Additional audio processing capabilities.\n",
    "## Google Cloud Text-to-Speech and Speech-to-Text APIs: Converting text to speech and vice versa.\n",
    "## OpenAI's GPT-2 Medium Model: A powerful text generation model known for its effectiveness in various NLP tasks.\n",
    "## EleutherAI's GPT-Neo 1.3B Model: An open-source alternative to GPT-3, featuring 1.3 billion parameters, tailored for generating context-aware responses.\n",
    "## GPT-NEO: Additional models from the GPT-NEO series, enhancing the assistant's conversational abilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd7f527",
   "metadata": {},
   "source": [
    "# Features\n",
    "\n",
    "## Audio Interaction\n",
    "\n",
    "### Recording and Playback: Using sounddevice and pyaudio for capturing and playing audio.\n",
    "### WAV File Handling: Utilizing wavio for reading and writing audio files.\n",
    "\n",
    "## AI-Driven Conversation\n",
    "\n",
    "### Speech Processing: Google Cloud APIs for transforming speech to text and text to speech.\n",
    "\n",
    "### Advanced NLP Models:\n",
    "#### GPT-2 Medium: Leveraged for high-quality text generation.\n",
    "#### GPT-Neo 1.3B: Used for its large-scale, efficient natural language understanding and generation.\n",
    "#### Other GPT-NEO Variants: Experimentation with various models to optimize conversational responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eea5fac",
   "metadata": {},
   "source": [
    "## Long-Term Vision\n",
    "The ultimate goal is to create a tool that not only assists healthcare professionals in their day-to-day tasks but also contributes to improved patient outcomes. The incorporation of AI-AudioResponse Assistant into healthcare settings could mark a significant advancement in how technology is utilized for patient care and medical record-keeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ed2cdac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sounddevice\n",
      "  Downloading sounddevice-0.4.6-py3-none-win_amd64.whl (199 kB)\n",
      "     -------------------------------------- 199.7/199.7 kB 3.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\shukl\\anaconda3\\lib\\site-packages (1.23.5)\n",
      "Collecting wavio\n",
      "  Downloading wavio-0.0.7-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting pygame\n",
      "  Downloading pygame-2.5.1-cp310-cp310-win_amd64.whl (10.6 MB)\n",
      "     --------------------------------------- 10.6/10.6 MB 31.2 MB/s eta 0:00:00\n",
      "Collecting google-cloud-texttospeech\n",
      "  Downloading google_cloud_texttospeech-2.14.1-py2.py3-none-any.whl (118 kB)\n",
      "     ---------------------------------------- 119.0/119.0 kB ? eta 0:00:00\n",
      "Collecting google-cloud-speech\n",
      "  Downloading google_cloud_speech-2.21.0-py2.py3-none-any.whl (273 kB)\n",
      "     ---------------------------------------- 273.7/273.7 kB ? eta 0:00:00\n",
      "Requirement already satisfied: openai in c:\\users\\shukl\\anaconda3\\lib\\site-packages (0.27.8)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\shukl\\anaconda3\\lib\\site-packages (from sounddevice) (1.15.1)\n",
      "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0\n",
      "  Downloading google_api_core-2.11.1-py3-none-any.whl (120 kB)\n",
      "     -------------------------------------- 120.5/120.5 kB 6.9 MB/s eta 0:00:00\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.0\n",
      "  Downloading proto_plus-1.22.3-py3-none-any.whl (48 kB)\n",
      "     ---------------------------------------- 48.1/48.1 kB ? eta 0:00:00\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5\n",
      "  Downloading protobuf-4.24.1-cp310-abi3-win_amd64.whl (430 kB)\n",
      "     ------------------------------------- 430.4/430.4 kB 26.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\shukl\\anaconda3\\lib\\site-packages (from openai) (2.28.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\shukl\\anaconda3\\lib\\site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shukl\\anaconda3\\lib\\site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\shukl\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice) (2.21)\n",
      "Collecting google-auth<3.0.dev0,>=2.14.1\n",
      "  Downloading google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "     ------------------------------------- 181.8/181.8 kB 11.4 MB/s eta 0:00:00\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB)\n",
      "     ------------------------------------- 227.6/227.6 kB 13.6 MB/s eta 0:00:00\n",
      "Collecting grpcio<2.0dev,>=1.33.2\n",
      "  Downloading grpcio-1.57.0-cp310-cp310-win_amd64.whl (4.3 MB)\n",
      "     ---------------------------------------- 4.3/4.3 MB 27.4 MB/s eta 0:00:00\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2\n",
      "  Downloading grpcio_status-1.57.0-py3-none-any.whl (5.1 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shukl\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\shukl\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shukl\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shukl\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\shukl\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shukl\\anaconda3\\lib\\site-packages (from aiohttp->openai) (22.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\shukl\\anaconda3\\lib\\site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shukl\\anaconda3\\lib\\site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\shukl\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shukl\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\shukl\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->openai) (0.4.5)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\shukl\\appdata\\roaming\\python\\python310\\site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-texttospeech) (1.16.0)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\shukl\\anaconda3\\lib\\site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-texttospeech) (0.2.8)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\shukl\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-texttospeech) (0.4.8)\n",
      "Installing collected packages: wavio, rsa, pygame, protobuf, grpcio, cachetools, sounddevice, proto-plus, googleapis-common-protos, google-auth, grpcio-status, google-api-core, google-cloud-texttospeech, google-cloud-speech\n",
      "Successfully installed cachetools-5.3.1 google-api-core-2.11.1 google-auth-2.22.0 google-cloud-speech-2.21.0 google-cloud-texttospeech-2.14.1 googleapis-common-protos-1.60.0 grpcio-1.57.0 grpcio-status-1.57.0 proto-plus-1.22.3 protobuf-4.24.1 pygame-2.5.1 rsa-4.9 sounddevice-0.4.6 wavio-0.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install sounddevice numpy wavio pygame google-cloud-texttospeech google-cloud-speech openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff767ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyaudioNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading PyAudio-0.2.13-cp310-cp310-win_amd64.whl (164 kB)\n",
      "     -------------------------------------- 164.1/164.1 kB 3.3 MB/s eta 0:00:00\n",
      "Installing collected packages: pyaudio\n",
      "Successfully installed pyaudio-0.2.13\n"
     ]
    }
   ],
   "source": [
    "pip install pyaudio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e3185",
   "metadata": {},
   "source": [
    "# Setting Up Audio Recording and Playback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d298d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "\n",
    "SAMPLE_RATE = 44100\n",
    "CHANNELS = 1\n",
    "DTYPE = np.int16\n",
    "SECONDS = 5  # Duration of recording\n",
    "\n",
    "def record_audio(filename=\"user_input.wav\"):\n",
    "    print(\"Recording...\")\n",
    "    audio_data = sd.rec(int(SECONDS * SAMPLE_RATE), samplerate=SAMPLE_RATE, channels=CHANNELS, dtype=DTYPE)\n",
    "    sd.wait()  # Wait until recording is done\n",
    "    wavio.write(filename, audio_data, SAMPLE_RATE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e2a2ad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "\n",
    "def play_audio(filename):\n",
    "    pygame.init()\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(filename)\n",
    "    pygame.mixer.music.play()\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        pygame.time.Clock().tick(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "94cbce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import texttospeech\n",
    "\n",
    "def text_to_speech(text):\n",
    "    client = texttospeech.TextToSpeechClient()\n",
    "    input_text = texttospeech.SynthesisInput(text=text)\n",
    "    voice = texttospeech.VoiceSelectionParams(\n",
    "        language_code=\"en-US\",\n",
    "        ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL,\n",
    "    )\n",
    "    audio_config = texttospeech.AudioConfig(\n",
    "        audio_encoding=texttospeech.AudioEncoding.MP3\n",
    "    )\n",
    "    response = client.synthesize_speech(input=input_text, voice=voice, audio_config=audio_config)\n",
    "\n",
    "    with open(\"output.mp3\", \"wb\") as out:\n",
    "        out.write(response.audio_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "32b3d6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import speech\n",
    "\n",
    "def speech_to_text(audio_file_path):\n",
    "    client = speech.SpeechClient()\n",
    "    with open(audio_file_path, \"rb\") as audio_file:\n",
    "        content = audio_file.read()\n",
    "    audio = speech.RecognitionAudio(content=content)\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=44100,\n",
    "        language_code=\"en-US\",\n",
    "    )\n",
    "    response = client.recognize(config=config, audio=audio)\n",
    "    return response.results[0].alternatives[0].transcript\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7425d3d5",
   "metadata": {},
   "source": [
    "# Setting up the AI Model (pretrained GPT2- medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cc7438c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Initialize tokenizer and model outside the function to avoid reloading\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "# Introductory context for the conversation\n",
    "intro_context = \"Meet Kylo, your personal assistant. Kylo is here to help you with your tasks.\\n\"\n",
    "\n",
    "def chat_with_gpt_local(prompt):\n",
    "    prompt_with_intro = intro_context + \"[User] \" + prompt + \" [Kylo]\"\n",
    "    input_ids = tokenizer.encode(prompt_with_intro, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length=200, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return decoded_output.split('[Kylo]')[-1].strip()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "043462f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kylo_assistant():\n",
    "    while True:\n",
    "        # Step 1: Record user's voice\n",
    "        record_audio()\n",
    "\n",
    "        # Step 2: Convert the recorded voice to text\n",
    "        text_input = speech_to_text('user_input.wav')\n",
    "        print(f\"You said: {text_input}\")\n",
    "        \n",
    "        prompt = f\"[User] {text_input} [Kylo]\"\n",
    "\n",
    "        # Step 3: Get a response from ChatGPT\n",
    "        response = chat_with_gpt_local(prompt)\n",
    "        print(f\"Kylo: {response}\")\n",
    "\n",
    "        # Step 4: Convert the response to audio\n",
    "        text_to_speech(response)\n",
    "\n",
    "        # Step 5: Play the response audio\n",
    "        play_audio('output.mp3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4685f5d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "You said: hello\n",
      "Kylo: [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [User] [\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [71], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mkylo_assistant\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [70], line 20\u001b[0m, in \u001b[0;36mkylo_assistant\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m text_to_speech(response)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Step 5: Play the response audio\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[43mplay_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput.mp3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [61], line 9\u001b[0m, in \u001b[0;36mplay_audio\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      7\u001b[0m pygame\u001b[38;5;241m.\u001b[39mmixer\u001b[38;5;241m.\u001b[39mmusic\u001b[38;5;241m.\u001b[39mplay()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m pygame\u001b[38;5;241m.\u001b[39mmixer\u001b[38;5;241m.\u001b[39mmusic\u001b[38;5;241m.\u001b[39mget_busy():\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mClock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kylo_assistant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "780e674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"****.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "980b85fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "You said: hello introduce yourself\n",
      "Kylo: hello introduce yourself, and what's your name?\"\n",
      "\n",
      "\"I'm a student at the University of California, Berkeley.\"\n",
      "\n",
      "\"I'm a student at the University of California, Berkeley.\"\n",
      "\n",
      "\"I'm a student at the University of California, Berkeley.\"\n",
      "\n",
      "\"I'm a student at the University of California, Berkeley.\"\n",
      "\n",
      "\"I'm a student at the University of California, Berkeley.\"\n",
      "\n",
      "\"I'm a student at the University of California, Berkeley.\"\n",
      "\n",
      "\"I'm a student at the University of California, Berkeley.\"\n",
      "\n",
      "\"I'm a student at the University of California, Berkeley.\"\n",
      "\n",
      "\"I'm a student at the University of California, Berkeley.\"\n",
      "\n",
      "\"I'm a student at the University of California, Berkeley.\"\n",
      "\n",
      "\"I'm a student at the University of California, Berkeley.\"\n",
      "\n",
      "\"I'm a student at the University of California, Berkeley.\"\n",
      "\n",
      "\"I'm a student at the University\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [73], line 91\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Run the assistant\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 91\u001b[0m     \u001b[43mkylo_assistant\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [73], line 87\u001b[0m, in \u001b[0;36mkylo_assistant\u001b[1;34m()\u001b[0m\n\u001b[0;32m     84\u001b[0m output_file \u001b[38;5;241m=\u001b[39m text_to_speech(kylo_response)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Play the audio response\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m \u001b[43mplay_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [73], line 32\u001b[0m, in \u001b[0;36mplay_audio\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     30\u001b[0m pygame\u001b[38;5;241m.\u001b[39mmixer\u001b[38;5;241m.\u001b[39mmusic\u001b[38;5;241m.\u001b[39mplay()\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m pygame\u001b[38;5;241m.\u001b[39mmixer\u001b[38;5;241m.\u001b[39mmusic\u001b[38;5;241m.\u001b[39mget_busy():\n\u001b[1;32m---> 32\u001b[0m     \u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mClock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "from google.cloud import texttospeech\n",
    "from google.cloud import speech\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import pygame\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize tokenizer and model outside the function to avoid reloading\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "# Audio recording settings\n",
    "SAMPLE_RATE = 44100\n",
    "CHANNELS = 1\n",
    "DTYPE = np.int16\n",
    "SECONDS = 5  # Duration of recording\n",
    "\n",
    "def record_audio(filename=\"user_input.wav\"):\n",
    "    print(\"Recording...\")\n",
    "    audio_data = sd.rec(int(SECONDS * SAMPLE_RATE), samplerate=SAMPLE_RATE, channels=CHANNELS, dtype=DTYPE)\n",
    "    sd.wait()\n",
    "    wavio.write(filename, audio_data, SAMPLE_RATE)\n",
    "\n",
    "def play_audio(filename):\n",
    "    pygame.init()\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(filename)\n",
    "    pygame.mixer.music.play()\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        pygame.time.Clock().tick(10)\n",
    "\n",
    "def text_to_speech(text):\n",
    "    client = texttospeech.TextToSpeechClient()\n",
    "    input_text = texttospeech.SynthesisInput(text=text)\n",
    "    voice = texttospeech.VoiceSelectionParams(\n",
    "        language_code=\"en-US\",\n",
    "        ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL,\n",
    "    )\n",
    "    audio_config = texttospeech.AudioConfig(\n",
    "        audio_encoding=texttospeech.AudioEncoding.MP3\n",
    "    )\n",
    "    response = client.synthesize_speech(input=input_text, voice=voice, audio_config=audio_config)\n",
    "    \n",
    "    filename = f\"output_{datetime.now().strftime('%Y%m%d%H%M%S')}.mp3\"\n",
    "    with open(filename, \"wb\") as out:\n",
    "        out.write(response.audio_content)\n",
    "    return filename\n",
    "\n",
    "def speech_to_text(audio_file_path):\n",
    "    client = speech.SpeechClient()\n",
    "    with open(audio_file_path, \"rb\") as audio_file:\n",
    "        content = audio_file.read()\n",
    "    audio = speech.RecognitionAudio(content=content)\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=44100,\n",
    "        language_code=\"en-US\",\n",
    "    )\n",
    "    response = client.recognize(config=config, audio=audio)\n",
    "    return response.results[0].alternatives[0].transcript\n",
    "\n",
    "def chat_with_gpt_local(prompt):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length=200, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return decoded_output\n",
    "\n",
    "def kylo_assistant():\n",
    "    while True:\n",
    "        # Record the user's voice\n",
    "        record_audio()\n",
    "        \n",
    "        # Convert the voice recording to text\n",
    "        user_input = speech_to_text('user_input.wav')\n",
    "        print(f\"You said: {user_input}\")\n",
    "        \n",
    "        # Generate Kylo's text response\n",
    "        kylo_response = chat_with_gpt_local(user_input)\n",
    "        print(f\"Kylo: {kylo_response}\")\n",
    "        \n",
    "        # Convert Kylo's text response to audio\n",
    "        output_file = text_to_speech(kylo_response)\n",
    "        \n",
    "        # Play the audio response\n",
    "        play_audio(output_file)\n",
    "\n",
    "# Run the assistant\n",
    "if __name__ == \"__main__\":\n",
    "    kylo_assistant()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1cd4fae1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "You said: what is the weather in Indianapolis\n",
      "Kylo: it's sunny.\n",
      "[User] what is the weather in Indianapolis\n",
      "Recording...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [75], line 97\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Run the assistant\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 97\u001b[0m     \u001b[43mkylo_assistant\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [75], line 82\u001b[0m, in \u001b[0;36mkylo_assistant\u001b[1;34m()\u001b[0m\n\u001b[0;32m     79\u001b[0m record_audio()\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Convert voice to text\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m text_input \u001b[38;5;241m=\u001b[39m \u001b[43mspeech_to_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser_input.wav\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou said: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Get a response from ChatGPT\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [75], line 58\u001b[0m, in \u001b[0;36mspeech_to_text\u001b[1;34m(audio_file_path)\u001b[0m\n\u001b[0;32m     52\u001b[0m config \u001b[38;5;241m=\u001b[39m speech\u001b[38;5;241m.\u001b[39mRecognitionConfig(\n\u001b[0;32m     53\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mspeech\u001b[38;5;241m.\u001b[39mRecognitionConfig\u001b[38;5;241m.\u001b[39mAudioEncoding\u001b[38;5;241m.\u001b[39mLINEAR16,\n\u001b[0;32m     54\u001b[0m     sample_rate_hertz\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m44100\u001b[39m,\n\u001b[0;32m     55\u001b[0m     language_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men-US\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     56\u001b[0m )\n\u001b[0;32m     57\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mrecognize(config\u001b[38;5;241m=\u001b[39mconfig, audio\u001b[38;5;241m=\u001b[39maudio)\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39malternatives[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtranscript\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\proto\\marshal\\collections\\repeated.py:125\u001b[0m, in \u001b[0;36mRepeatedComposite.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_marshal\u001b[38;5;241m.\u001b[39mto_python(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pb_type, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpb\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "import pygame\n",
    "from google.cloud import texttospeech\n",
    "from google.cloud import speech\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Audio recording settings\n",
    "SAMPLE_RATE = 44100\n",
    "CHANNELS = 1\n",
    "DTYPE = np.int16\n",
    "SECONDS = 5\n",
    "\n",
    "# Record audio to a file\n",
    "def record_audio(filename=\"user_input.wav\"):\n",
    "    print(\"Recording...\")\n",
    "    audio_data = sd.rec(int(SECONDS * SAMPLE_RATE), samplerate=SAMPLE_RATE, channels=CHANNELS, dtype=DTYPE)\n",
    "    sd.wait()\n",
    "    wavio.write(filename, audio_data, SAMPLE_RATE)\n",
    "\n",
    "# Play audio from a file\n",
    "def play_audio(filename):\n",
    "    pygame.init()\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(filename)\n",
    "    pygame.mixer.music.play()\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        pygame.time.Clock().tick(10)\n",
    "\n",
    "# Text-to-speech function\n",
    "def text_to_speech(text):\n",
    "    client = texttospeech.TextToSpeechClient()\n",
    "    input_text = texttospeech.SynthesisInput(text=text)\n",
    "    voice = texttospeech.VoiceSelectionParams(\n",
    "        language_code=\"en-US\",\n",
    "        ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL,\n",
    "    )\n",
    "    audio_config = texttospeech.AudioConfig(\n",
    "        audio_encoding=texttospeech.AudioEncoding.MP3\n",
    "    )\n",
    "    response = client.synthesize_speech(input=input_text, voice=voice, audio_config=audio_config)\n",
    "    with open(\"output11.mp3\", \"wb\") as out:\n",
    "        out.write(response.audio_content)\n",
    "\n",
    "# Speech-to-text function\n",
    "def speech_to_text(audio_file_path):\n",
    "    client = speech.SpeechClient()\n",
    "    with open(audio_file_path, \"rb\") as audio_file:\n",
    "        content = audio_file.read()\n",
    "    audio = speech.RecognitionAudio(content=content)\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=44100,\n",
    "        language_code=\"en-US\",\n",
    "    )\n",
    "    response = client.recognize(config=config, audio=audio)\n",
    "    return response.results[0].alternatives[0].transcript\n",
    "\n",
    "# Initialize GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "# Introductory context for the conversation\n",
    "intro_context = \"Meet Kylo, your personal assistant. Kylo is here to help you with your tasks.\\n\"\n",
    "\n",
    "# Chat with GPT-2\n",
    "def chat_with_gpt_local(prompt):\n",
    "    prompt_with_intro = intro_context + \"[User] \" + prompt + \" [Kylo]\"\n",
    "    input_ids = tokenizer.encode(prompt_with_intro, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length=200, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return decoded_output.split('[Kylo]')[-1].strip()\n",
    "\n",
    "# Kylo Assistant Main Function\n",
    "def kylo_assistant():\n",
    "    while True:\n",
    "        # Record user's voice\n",
    "        record_audio()\n",
    "\n",
    "        # Convert voice to text\n",
    "        text_input = speech_to_text('user_input.wav')\n",
    "        print(f\"You said: {text_input}\")\n",
    "\n",
    "        # Get a response from ChatGPT\n",
    "        kylo_response = chat_with_gpt_local(text_input)\n",
    "        print(f\"Kylo: {kylo_response}\")\n",
    "\n",
    "        # Convert Kylo's text response to audio\n",
    "        text_to_speech(kylo_response)\n",
    "\n",
    "        # Play the audio response\n",
    "        play_audio(\"output11.mp3\")\n",
    "\n",
    "# Run the assistant\n",
    "if __name__ == \"__main__\":\n",
    "    kylo_assistant()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a38a6eb",
   "metadata": {},
   "source": [
    "This time gpt2-medium model performed better comparatively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43f2c01",
   "metadata": {},
   "source": [
    "# Using the GPT-NEO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c88b7fcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You said: hello how are you\n",
      "Kylo: hello how are you\n",
      "i'm so excited to be here today\n",
      "and i'm so excited to be here today\n",
      "and i'm so excited to be here today\n",
      "and i'm so excited to be here today\n",
      "and i'm so excited to be here today\n",
      "and i'm so excited to be here today\n",
      "and i'm so excited to be here today\n",
      "and i'm so excited to be here today\n",
      "and i'm so excited to be here today\n",
      "and i'm so excited to be here today\n",
      "and i'm so excited to be here today\n",
      "and i'm so excited to be here today\n",
      "and i'm so excited to be here today\n",
      "and i'm so excited to be here today\n",
      "and i'm so excited to\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [77], line 75\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Run the assistant\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 75\u001b[0m     \u001b[43mkylo_assistant\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [77], line 71\u001b[0m, in \u001b[0;36mkylo_assistant\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKylo: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     70\u001b[0m text_to_speech(response)\n\u001b[1;32m---> 71\u001b[0m \u001b[43mplay_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput.mp3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [77], line 26\u001b[0m, in \u001b[0;36mplay_audio\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     24\u001b[0m pygame\u001b[38;5;241m.\u001b[39mmixer\u001b[38;5;241m.\u001b[39mmusic\u001b[38;5;241m.\u001b[39mplay()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m pygame\u001b[38;5;241m.\u001b[39mmixer\u001b[38;5;241m.\u001b[39mmusic\u001b[38;5;241m.\u001b[39mget_busy():\n\u001b[1;32m---> 26\u001b[0m     \u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mClock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "from google.cloud import texttospeech, speech\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pygame\n",
    "\n",
    "# Record audio\n",
    "def record_audio(filename=\"user_input.wav\"):\n",
    "    SAMPLE_RATE = 44100  # Sample rate\n",
    "    CHANNELS = 1  # Number of channels (1=mono, 2=stereo)\n",
    "    DTYPE = np.int16  # Audio format\n",
    "    SECONDS = 5  # Duration of recording\n",
    "    print(\"Recording...\")\n",
    "    audio_data = sd.rec(int(SECONDS * SAMPLE_RATE), samplerate=SAMPLE_RATE, channels=CHANNELS, dtype=DTYPE)\n",
    "    sd.wait()\n",
    "    wavio.write(filename, audio_data, SAMPLE_RATE)\n",
    "\n",
    "# Play audio\n",
    "def play_audio(filename):\n",
    "    pygame.init()\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(filename)\n",
    "    pygame.mixer.music.play()\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        pygame.time.Clock().tick(10)\n",
    "\n",
    "# Text-to-speech\n",
    "def text_to_speech(text):\n",
    "    client = texttospeech.TextToSpeechClient()\n",
    "    input_text = texttospeech.SynthesisInput(text=text)\n",
    "    voice = texttospeech.VoiceSelectionParams(language_code=\"en-US\", ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL)\n",
    "    audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)\n",
    "    response = client.synthesize_speech(input=input_text, voice=voice, audio_config=audio_config)\n",
    "    with open(\"output.mp3\", \"wb\") as out:\n",
    "        out.write(response.audio_content)\n",
    "\n",
    "# Speech-to-text\n",
    "def speech_to_text(audio_file_path):\n",
    "    client = speech.SpeechClient()\n",
    "    with open(audio_file_path, \"rb\") as audio_file:\n",
    "        content = audio_file.read()\n",
    "    audio = speech.RecognitionAudio(content=content)\n",
    "    config = speech.RecognitionConfig(encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16, sample_rate_hertz=44100, language_code=\"en-US\")\n",
    "    response = client.recognize(config=config, audio=audio)\n",
    "    if len(response.results) > 0:\n",
    "        return response.results[0].alternatives[0].transcript\n",
    "    else:\n",
    "        return \"Could not understand audio.\"\n",
    "\n",
    "# Initialize GPT-Neo\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "\n",
    "# Chat with GPT-Neo\n",
    "def chat_with_gpt(prompt):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(input_ids, max_length=150, num_return_sequences=1)\n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return decoded_output\n",
    "\n",
    "# Kylo assistant function\n",
    "def kylo_assistant():\n",
    "    while True:\n",
    "        record_audio()\n",
    "        text_input = speech_to_text('user_input.wav')\n",
    "        print(f\"You said: {text_input}\")\n",
    "        response = chat_with_gpt(text_input)\n",
    "        print(f\"Kylo: {response}\")\n",
    "        text_to_speech(response)\n",
    "        play_audio('output.mp3')\n",
    "\n",
    "# Run the assistant\n",
    "if __name__ == \"__main__\":\n",
    "    kylo_assistant()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90073677",
   "metadata": {},
   "source": [
    "So here the AI model got into a loop of answer and repeating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "afa6045b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "You said: hello how are you\n",
      "Kylo: hello how are you\n",
      "i'm so excited to be here today\n",
      "and i'm here with my\n",
      "favorite author\n",
      "who is also my favorite\n",
      "author\n",
      "is the one and only\n",
      "jane austen\n",
      "so i've been reading her\n",
      "books for a long time\n",
      "but i never really\n",
      "thought about her as a\n",
      "writer until i started\n",
      "reading her books\n",
      "in the last couple of years\n",
      "because i was so\n",
      "impressed with her writing\n",
      "that i decided to\n",
      "start reading all of her novels\n",
      "which i did\n",
      "for a couple years and i\n",
      "started to really like\n",
      "her writing style\n",
      "as well as her characters\n",
      "she's so unique and\n",
      "different and she's\n",
      "written so many different\n",
      "characters\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'output.mp3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [78], line 93\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Run the assistant\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mkylo_assistant\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [78], line 88\u001b[0m, in \u001b[0;36mkylo_assistant\u001b[1;34m()\u001b[0m\n\u001b[0;32m     86\u001b[0m response \u001b[38;5;241m=\u001b[39m chat_with_gpt(text_input)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKylo: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 88\u001b[0m \u001b[43mtext_to_speech\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m play_audio(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput.mp3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn [78], line 41\u001b[0m, in \u001b[0;36mtext_to_speech\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     37\u001b[0m audio_config \u001b[38;5;241m=\u001b[39m texttospeech\u001b[38;5;241m.\u001b[39mAudioConfig(\n\u001b[0;32m     38\u001b[0m     audio_encoding\u001b[38;5;241m=\u001b[39mtexttospeech\u001b[38;5;241m.\u001b[39mAudioEncoding\u001b[38;5;241m.\u001b[39mMP3\n\u001b[0;32m     39\u001b[0m )\n\u001b[0;32m     40\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39msynthesize_speech(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39minput_text, voice\u001b[38;5;241m=\u001b[39mvoice, audio_config\u001b[38;5;241m=\u001b[39maudio_config)\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput.mp3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m out:\n\u001b[0;32m     42\u001b[0m     out\u001b[38;5;241m.\u001b[39mwrite(response\u001b[38;5;241m.\u001b[39maudio_content)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'output.mp3'"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "from google.cloud import texttospeech, speech\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pygame\n",
    "\n",
    "# Audio Recording\n",
    "SAMPLE_RATE = 44100\n",
    "CHANNELS = 1\n",
    "DTYPE = np.int16\n",
    "SECONDS = 5  # Duration of recording\n",
    "\n",
    "def record_audio(filename=\"user_input.wav\"):\n",
    "    print(\"Recording...\")\n",
    "    audio_data = sd.rec(int(SECONDS * SAMPLE_RATE), samplerate=SAMPLE_RATE, channels=CHANNELS, dtype=DTYPE)\n",
    "    sd.wait()\n",
    "    wavio.write(filename, audio_data, SAMPLE_RATE)\n",
    "\n",
    "# Audio Playback\n",
    "def play_audio(filename):\n",
    "    pygame.init()\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(filename)\n",
    "    pygame.mixer.music.play()\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        pygame.time.Clock().tick(10)\n",
    "\n",
    "# Text to Speech\n",
    "def text_to_speech(text):\n",
    "    client = texttospeech.TextToSpeechClient()\n",
    "    input_text = texttospeech.SynthesisInput(text=text)\n",
    "    voice = texttospeech.VoiceSelectionParams(\n",
    "        language_code=\"en-US\",\n",
    "        ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL,\n",
    "    )\n",
    "    audio_config = texttospeech.AudioConfig(\n",
    "        audio_encoding=texttospeech.AudioEncoding.MP3\n",
    "    )\n",
    "    response = client.synthesize_speech(input=input_text, voice=voice, audio_config=audio_config)\n",
    "    with open(\"output.mp3\", \"wb\") as out:\n",
    "        out.write(response.audio_content)\n",
    "\n",
    "# Speech to Text\n",
    "def speech_to_text(audio_file_path):\n",
    "    client = speech.SpeechClient()\n",
    "    with open(audio_file_path, \"rb\") as audio_file:\n",
    "        content = audio_file.read()\n",
    "    audio = speech.RecognitionAudio(content=content)\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=44100,\n",
    "        language_code=\"en-US\",\n",
    "    )\n",
    "    response = client.recognize(config=config, audio=audio)\n",
    "    return response.results[0].alternatives[0].transcript\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "\n",
    "# Chat with GPT-Neo\n",
    "def chat_with_gpt(prompt):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(input_ids, \n",
    "                            max_length=150, \n",
    "                            num_return_sequences=1, \n",
    "                            pad_token_id=tokenizer.eos_token_id, \n",
    "                            no_repeat_ngram_size=2,\n",
    "                            top_p=0.95,\n",
    "                            top_k=50)\n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return decoded_output\n",
    "\n",
    "# Kylo assistant function\n",
    "def kylo_assistant():\n",
    "    while True:\n",
    "        record_audio()\n",
    "        text_input = speech_to_text('user_input.wav')\n",
    "        print(f\"You said: {text_input}\")\n",
    "        \n",
    "        if text_input.lower() in [\"exit\", \"goodbye\", \"bye\"]:\n",
    "            print(\"Kylo: Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        response = chat_with_gpt(text_input)\n",
    "        print(f\"Kylo: {response}\")\n",
    "        text_to_speech(response)\n",
    "        play_audio('output.mp3')\n",
    "\n",
    "# Run the assistant\n",
    "if __name__ == \"__main__\":\n",
    "    kylo_assistant()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3406cd43",
   "metadata": {},
   "source": [
    "So here Eleuther gpt-neo-1.3B AI model answered in a story form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3a162d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "import pygame\n",
    "from google.cloud import texttospeech\n",
    "from google.cloud import speech\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Recording Audio\n",
    "def record_audio(filename=\"user_input.wav\"):\n",
    "    print(\"Recording...\")\n",
    "    SAMPLE_RATE = 44100  # Hertz\n",
    "    DURATION = 5  # Seconds\n",
    "    audio_data = sd.rec(int(SAMPLE_RATE * DURATION), samplerate=SAMPLE_RATE, channels=1, dtype=\"int16\")\n",
    "    sd.wait()\n",
    "    wavio.write(filename, audio_data, SAMPLE_RATE, sampwidth=2)\n",
    "\n",
    "# Text to Speech\n",
    "def text_to_speech(text, filename):\n",
    "    client = texttospeech.TextToSpeechClient()\n",
    "    input_text = texttospeech.SynthesisInput(text=text)\n",
    "    voice = texttospeech.VoiceSelectionParams(language_code=\"en-US\", ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL)\n",
    "    audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)\n",
    "    response = client.synthesize_speech(input=input_text, voice=voice, audio_config=audio_config)\n",
    "    with open(filename, \"wb\") as out:\n",
    "        out.write(response.audio_content)\n",
    "\n",
    "# Speech to Text\n",
    "def speech_to_text(audio_file_path):\n",
    "    client = speech.SpeechClient()\n",
    "    with open(audio_file_path, \"rb\") as audio_file:\n",
    "        content = audio_file.read()\n",
    "    audio = speech.RecognitionAudio(content=content)\n",
    "    config = speech.RecognitionConfig(encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16, sample_rate_hertz=44100, language_code=\"en-US\")\n",
    "    response = client.recognize(config=config, audio=audio)\n",
    "    return response.results[0].alternatives[0].transcript\n",
    "\n",
    "# Play Audio File\n",
    "def play_audio(filename):\n",
    "    pygame.init()\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(filename)\n",
    "    pygame.mixer.music.play()\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        pygame.time.Clock().tick(10)\n",
    "\n",
    "# Initialize GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "# Chat with GPT-2\n",
    "def chat_with_gpt(prompt):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    # Add more controls for text generation\n",
    "    output = model.generate(input_ids, max_length=50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id, top_p=0.92, top_k=50)\n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return decoded_output\n",
    "\n",
    "# Main Assistant Loop\n",
    "def kylo_assistant():\n",
    "    while True:\n",
    "        # Record user voice\n",
    "        record_audio()\n",
    "        \n",
    "        # Convert voice to text\n",
    "        text_input = speech_to_text('user_input.wav')\n",
    "        print(f\"You said: {text_input}\")\n",
    "\n",
    "        # Get a response from GPT-2\n",
    "        prompt = f\"{text_input}\"\n",
    "        response = chat_with_gpt(prompt)\n",
    "        print(f\"Kylo: {response}\")\n",
    "\n",
    "        # Generate unique filename for each output\n",
    "        unique_filename = f\"output_{np.random.randint(1, 10000)}.mp3\"\n",
    "\n",
    "        # Convert text to audio\n",
    "        text_to_speech(response, unique_filename)\n",
    "        \n",
    "        # Play the audio\n",
    "        play_audio(unique_filename)\n",
    "\n",
    "        # Stop the audio and delete the unique audio file to prevent PermissionError\n",
    "        pygame.mixer.music.stop()\n",
    "        os.remove(unique_filename)\n",
    "\n",
    "# Function to play audio\n",
    "def play_audio(filename):\n",
    "    pygame.init()\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(filename)\n",
    "    pygame.mixer.music.play()\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        pygame.time.Clock().tick(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1e793f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3222e1ec",
   "metadata": {},
   "source": [
    "# Trying to get good answers from GPT2 medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "adc2f6ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "You said: hello\n",
      "Kylo: hello, my name is John, and I'm a writer. I'm a writer who writes about the world of science fiction\n",
      "Failed to delete the audio file. It might be in use.\n",
      "Recording...\n",
      "You said: what\n",
      "Kylo: what is the difference between a \"good\" and a \"bad\" person?\n",
      "\n",
      "A good person is someone who is good at\n",
      "Failed to delete the audio file. It might be in use.\n",
      "Recording...\n",
      "You said: hello\n",
      "Kylo: hello, my name is John, and I'm a writer. I'm a writer who writes about the world of science fiction\n",
      "Failed to delete the audio file. It might be in use.\n",
      "Recording...\n",
      "You said: can you give me examples\n",
      "Kylo: can you give me examples of how you've been able to do that?\"\n",
      "\n",
      "\"I've been able to do it because I've\n",
      "Failed to delete the audio file. It might be in use.\n",
      "Recording...\n",
      "You said: bye\n",
      "Kylo: bye, I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sor\n",
      "Failed to delete the audio file. It might be in use.\n",
      "Recording...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [91], line 104\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Run the assistant\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 104\u001b[0m     \u001b[43mkylo_assistant\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [91], line 79\u001b[0m, in \u001b[0;36mkylo_assistant\u001b[1;34m()\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkylo_assistant\u001b[39m():\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;66;03m# Record user voice\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m         \u001b[43mrecord_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;66;03m# Convert voice to text\u001b[39;00m\n\u001b[0;32m     82\u001b[0m         text_input \u001b[38;5;241m=\u001b[39m speech_to_text(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_input.wav\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn [91], line 21\u001b[0m, in \u001b[0;36mrecord_audio\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecording...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m audio_data \u001b[38;5;241m=\u001b[39m sd\u001b[38;5;241m.\u001b[39mrec(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m44100\u001b[39m), samplerate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m44100\u001b[39m, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint16\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m \u001b[43msd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m wavio\u001b[38;5;241m.\u001b[39mwrite(filename, audio_data, \u001b[38;5;241m44100\u001b[39m, sampwidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sounddevice.py:395\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(ignore_errors)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;124;03m\"\"\"Wait for `play()`/`rec()`/`playrec()` to be finished.\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \n\u001b[0;32m    381\u001b[0m \u001b[38;5;124;03mPlayback/recording can be stopped with a `KeyboardInterrupt`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m \n\u001b[0;32m    393\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _last_callback:\n\u001b[1;32m--> 395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_last_callback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sounddevice.py:2601\u001b[0m, in \u001b[0;36m_CallbackContext.wait\u001b[1;34m(self, ignore_errors)\u001b[0m\n\u001b[0;32m   2595\u001b[0m \u001b[38;5;124;03m\"\"\"Wait for finished_callback.\u001b[39;00m\n\u001b[0;32m   2596\u001b[0m \n\u001b[0;32m   2597\u001b[0m \u001b[38;5;124;03mCan be interrupted with a KeyboardInterrupt.\u001b[39;00m\n\u001b[0;32m   2598\u001b[0m \n\u001b[0;32m   2599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2600\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2601\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2602\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   2603\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mclose(ignore_errors)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import sounddevice as sd\n",
    "import wavio\n",
    "import pygame\n",
    "from google.cloud import texttospeech, speech\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Initialize the GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "# Initialize pygame for audio playback\n",
    "pygame.init()\n",
    "\n",
    "# Function to record audio\n",
    "def record_audio(filename=\"user_input.wav\"):\n",
    "    print(\"Recording...\")\n",
    "    audio_data = sd.rec(int(5 * 44100), samplerate=44100, channels=1, dtype=\"int16\")\n",
    "    sd.wait()\n",
    "    wavio.write(filename, audio_data, 44100, sampwidth=2)\n",
    "\n",
    "# Function to convert speech to text\n",
    "def speech_to_text(audio_file_path):\n",
    "    client = speech.SpeechClient()\n",
    "    with open(audio_file_path, \"rb\") as audio_file:\n",
    "        content = audio_file.read()\n",
    "    audio = speech.RecognitionAudio(content=content)\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=44100,\n",
    "        language_code=\"en-US\",\n",
    "    )\n",
    "    response = client.recognize(config=config, audio=audio)\n",
    "    return response.results[0].alternatives[0].transcript\n",
    "\n",
    "# Function to generate text from GPT-2\n",
    "def chat_with_gpt(text_input):\n",
    "    input_ids = tokenizer.encode(text_input, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length=100, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    # Limit the length of the generated text to avoid infinite loops\n",
    "    truncated_output = decoded_output[:100]\n",
    "    return truncated_output\n",
    "\n",
    "\n",
    "# Function to convert text to speech\n",
    "def text_to_speech(text, filename):\n",
    "    client = texttospeech.TextToSpeechClient()\n",
    "    input_text = texttospeech.SynthesisInput(text=text)\n",
    "    voice = texttospeech.VoiceSelectionParams(\n",
    "        language_code=\"en-US\",\n",
    "        ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL,\n",
    "    )\n",
    "    audio_config = texttospeech.AudioConfig(\n",
    "        audio_encoding=texttospeech.AudioEncoding.MP3\n",
    "    )\n",
    "    response = client.synthesize_speech(input=input_text, voice=voice, audio_config=audio_config)\n",
    "    with open(filename, \"wb\") as out:\n",
    "        out.write(response.audio_content)\n",
    "\n",
    "# Function to play audio\n",
    "def play_audio(filename):\n",
    "    pygame.mixer.music.load(filename)\n",
    "    pygame.mixer.music.play()\n",
    "    start_time = time.time()\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        pygame.time.Clock().tick(10)\n",
    "        # Exit the loop if audio has been playing for more than 10 seconds\n",
    "        if time.time() - start_time > 5:\n",
    "            break\n",
    "    pygame.mixer.music.stop()\n",
    "\n",
    "# Main function to run the assistant\n",
    "def kylo_assistant():\n",
    "    while True:\n",
    "        # Record user voice\n",
    "        record_audio()\n",
    "        \n",
    "        # Convert voice to text\n",
    "        text_input = speech_to_text('user_input.wav')\n",
    "        print(f\"You said: {text_input}\")\n",
    "\n",
    "        # Get a response from ChatGPT\n",
    "        response = chat_with_gpt(text_input)\n",
    "        print(f\"Kylo: {response}\")\n",
    "\n",
    "        # Convert Kylo's text response to audio\n",
    "        unique_filename = f\"output_{random.randint(1000, 9999)}.mp3\"\n",
    "        text_to_speech(response, unique_filename)\n",
    "\n",
    "        # Play the audio response\n",
    "        play_audio(unique_filename)\n",
    "\n",
    "        # Delete the unique audio file to prevent PermissionError\n",
    "        try:\n",
    "            os.remove(unique_filename)\n",
    "        except PermissionError:\n",
    "            print(\"Failed to delete the audio file. It might be in use.\")\n",
    "\n",
    "# Run the assistant\n",
    "if __name__ == \"__main__\":\n",
    "    kylo_assistant()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412bc4ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3e9e9b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "You said: hello how are you\n",
      "Kylo: hello how are you doing?\n",
      "\n",
      "I'm doing fine.\n",
      "Successfully deleted output_8156.mp3\n",
      "Recording...\n",
      "You said: what's your name\n",
      "Kylo: what's your name?\"\n",
      "\n",
      "\"I'm not sure,\" he said.\n",
      "Successfully deleted output_4093.mp3\n",
      "Recording...\n",
      "You said: bye\n",
      "Kylo: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "import pygame\n",
    "from google.cloud import texttospeech\n",
    "from google.cloud import speech\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Initialize GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "# Record audio\n",
    "def record_audio(filename=\"user_input.wav\"):\n",
    "    print(\"Recording...\")\n",
    "    audio_data = sd.rec(int(5 * 44100), samplerate=44100, channels=1, dtype=\"int16\")\n",
    "    sd.wait()\n",
    "    wavio.write(filename, audio_data, 44100, sampwidth=2)\n",
    "\n",
    "# Function to play audio\n",
    "def play_audio(filename):\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(filename)\n",
    "    pygame.mixer.music.play()\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        pygame.time.Clock().tick(10)\n",
    "    pygame.mixer.music.stop()\n",
    "    pygame.mixer.quit()\n",
    "\n",
    "# Text to speech\n",
    "def text_to_speech(text, filename):\n",
    "    client = texttospeech.TextToSpeechClient()\n",
    "    input_text = texttospeech.SynthesisInput(text=text)\n",
    "    voice = texttospeech.VoiceSelectionParams(language_code=\"en-US\", ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL)\n",
    "    audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)\n",
    "    response = client.synthesize_speech(input=input_text, voice=voice, audio_config=audio_config)\n",
    "    with open(filename, \"wb\") as out:\n",
    "        out.write(response.audio_content)\n",
    "\n",
    "# Speech to text\n",
    "def speech_to_text(audio_file_path):\n",
    "    client = speech.SpeechClient()\n",
    "    with open(audio_file_path, \"rb\") as audio_file:\n",
    "        content = audio_file.read()\n",
    "    audio = speech.RecognitionAudio(content=content)\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=44100,\n",
    "        language_code=\"en-US\",\n",
    "    )\n",
    "    response = client.recognize(config=config, audio=audio)\n",
    "    return response.results[0].alternatives[0].transcript if response.results else \"Could not understand audio\"\n",
    "\n",
    "# Chat with GPT-2\n",
    "def chat_with_gpt(text_input):\n",
    "    input_ids = tokenizer.encode(text_input, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length=50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    truncated_output = decoded_output.split(\".\")[0] + \".\"\n",
    "    return truncated_output\n",
    "\n",
    "# Main assistant function\n",
    "def kylo_assistant():\n",
    "    while True:\n",
    "        # Record user voice\n",
    "        record_audio()\n",
    "        \n",
    "        # Convert voice to text\n",
    "        text_input = speech_to_text('user_input.wav')\n",
    "        print(f\"You said: {text_input}\")\n",
    "        \n",
    "        # Check if the user said 'bye' to stop the assistant\n",
    "        if 'bye' in text_input.lower():\n",
    "            print(\"Kylo: Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Generate response\n",
    "        response = chat_with_gpt(text_input)\n",
    "        \n",
    "        print(f\"Kylo: {response}\")\n",
    "        \n",
    "        # Convert text to speech\n",
    "        unique_filename = f\"output_{random.randint(1000, 9999)}.mp3\"\n",
    "        text_to_speech(response, unique_filename)\n",
    "        \n",
    "        # Play the audio response\n",
    "        play_audio(unique_filename)\n",
    "        \n",
    "        # Delay to ensure the file is no longer in use\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Delete the unique audio file to prevent PermissionError\n",
    "        try:\n",
    "            os.remove(unique_filename)\n",
    "            print(f\"Successfully deleted {unique_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete the audio file. It might be in use.\")\n",
    "            \n",
    "# Run the assistant\n",
    "if __name__ == \"__main__\":\n",
    "    kylo_assistant()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d03f24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.1 (SDL 2.28.2, Python 3.10.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Initialize GPT-Neo\u001b[39;00m\n\u001b[0;32m     14\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEleutherAI/gpt-j-6B\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEleutherAI/gpt-j-6B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Record audio\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecord_audio\u001b[39m(filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_input.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:463\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    462\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    464\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    465\u001b[0m     )\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    469\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py:2326\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2324\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m-> 2326\u001b[0m     model, missing_keys, unexpected_keys, mismatched_keys, error_msgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[0;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2333\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2334\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2337\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2338\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_in_8bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2341\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2343\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[0;32m   2344\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py:2517\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, load_in_8bit)\u001b[0m\n\u001b[0;32m   2507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2508\u001b[0m     \u001b[38;5;66;03m# Whole checkpoint\u001b[39;00m\n\u001b[0;32m   2509\u001b[0m     mismatched_keys \u001b[38;5;241m=\u001b[39m _find_mismatched_keys(\n\u001b[0;32m   2510\u001b[0m         state_dict,\n\u001b[0;32m   2511\u001b[0m         model_state_dict,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2515\u001b[0m         ignore_mismatched_sizes,\n\u001b[0;32m   2516\u001b[0m     )\n\u001b[1;32m-> 2517\u001b[0m     error_msgs \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2518\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2519\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n\u001b[0;32m   2520\u001b[0m \n\u001b[0;32m   2521\u001b[0m     \u001b[38;5;66;03m# This should always be a list but, just to be sure.\u001b[39;00m\n\u001b[0;32m   2522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resolved_archive_file, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py:475\u001b[0m, in \u001b[0;36m_load_state_dict_into_model\u001b[1;34m(model_to_load, state_dict, start_prefix)\u001b[0m\n\u001b[0;32m    472\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    473\u001b[0m             load(child, state_dict, prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 475\u001b[0m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# Delete `state_dict` so it could be collected by GC earlier. Note that `state_dict` is a copy of the argument, so\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;66;03m# it's safe to delete it.\u001b[39;00m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py:473\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[1;34m(module, state_dict, prefix)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 473\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py:473\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[1;34m(module, state_dict, prefix)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 473\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "    \u001b[1;31m[... skipping similar frames: _load_state_dict_into_model.<locals>.load at line 473 (2 times)]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py:473\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[1;34m(module, state_dict, prefix)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 473\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py:469\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[1;34m(module, state_dict, prefix)\u001b[0m\n\u001b[0;32m    467\u001b[0m                     module\u001b[38;5;241m.\u001b[39m_load_from_state_dict(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 469\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_from_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1507\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[1;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1506\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m-> 1507\u001b[0m         \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_param\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m   1509\u001b[0m     error_msgs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhile copying the parameter named \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1510\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhose dimensions in the model are \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1511\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhose dimensions in the checkpoint are \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1512\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124man exception occurred : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1513\u001b[0m                       \u001b[38;5;241m.\u001b[39mformat(key, param\u001b[38;5;241m.\u001b[39msize(), input_param\u001b[38;5;241m.\u001b[39msize(), ex\u001b[38;5;241m.\u001b[39margs))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "import pygame\n",
    "from google.cloud import texttospeech\n",
    "from google.cloud import speech\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPTNeoForCausalLM, AutoTokenizer\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Initialize GPT-Neo\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "\n",
    "# Record audio\n",
    "def record_audio(filename=\"user_input.wav\"):\n",
    "    print(\"Recording...\")\n",
    "    audio_data = sd.rec(int(5 * 44100), samplerate=44100, channels=1, dtype=\"int16\")\n",
    "    sd.wait()\n",
    "    wavio.write(filename, audio_data, 44100, sampwidth=2)\n",
    "\n",
    "# Function to play audio\n",
    "def play_audio(filename):\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(filename)\n",
    "    pygame.mixer.music.play()\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        pygame.time.Clock().tick(10)\n",
    "    pygame.mixer.music.stop()\n",
    "    pygame.mixer.quit()\n",
    "\n",
    "# Text to speech\n",
    "def text_to_speech(text, filename):\n",
    "    client = texttospeech.TextToSpeechClient()\n",
    "    input_text = texttospeech.SynthesisInput(text=text)\n",
    "    voice = texttospeech.VoiceSelectionParams(language_code=\"en-US\", ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL)\n",
    "    audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)\n",
    "    response = client.synthesize_speech(input=input_text, voice=voice, audio_config=audio_config)\n",
    "    with open(filename, \"wb\") as out:\n",
    "        out.write(response.audio_content)\n",
    "\n",
    "# Speech to text\n",
    "def speech_to_text(audio_file_path):\n",
    "    client = speech.SpeechClient()\n",
    "    with open(audio_file_path, \"rb\") as audio_file:\n",
    "        content = audio_file.read()\n",
    "    audio = speech.RecognitionAudio(content=content)\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=44100,\n",
    "        language_code=\"en-US\",\n",
    "    )\n",
    "    response = client.recognize(config=config, audio=audio)\n",
    "    return response.results[0].alternatives[0].transcript if response.results else \"Could not understand audio\"\n",
    "\n",
    "# Chat with GPT-Neo\n",
    "def chat_with_gpt(text_input):\n",
    "    input_ids = tokenizer.encode(text_input, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length=50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    truncated_output = decoded_output.split(\".\")[0] + \".\"\n",
    "    return truncated_output\n",
    "\n",
    "# Main assistant function\n",
    "def kylo_assistant():\n",
    "    while True:\n",
    "        # Record user voice\n",
    "        record_audio()\n",
    "        \n",
    "        # Convert voice to text\n",
    "        text_input = speech_to_text('user_input.wav')\n",
    "        print(f\"You said: {text_input}\")\n",
    "        \n",
    "        # Check if the user said 'bye' to stop the assistant\n",
    "        if 'bye' in text_input.lower():\n",
    "            print(\"Kylo: Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Generate response\n",
    "        response = chat_with_gpt(text_input)\n",
    "        \n",
    "        print(f\"Kylo: {response}\")\n",
    "        \n",
    "        # Convert text to speech\n",
    "        unique_filename = f\"output_{random.randint(1000, 9999)}.mp3\"\n",
    "        text_to_speech(response, unique_filename)\n",
    "        \n",
    "        # Play the audio response\n",
    "        play_audio(unique_filename)\n",
    "        \n",
    "        # Delay to ensure the file is no longer in use\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Delete the unique audio file to prevent PermissionError\n",
    "        try:\n",
    "            os.remove(unique_filename)\n",
    "            print(f\"Successfully deleted {unique_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete the audio file. It might be in use.\")\n",
    "            \n",
    "# Run the assistant\n",
    "if __name__ == \"__main__\":\n",
    "    kylo_assistant()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "168b15fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyttsx3\n",
      "  Downloading pyttsx3-2.90-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: pywin32 in c:\\users\\shukl\\appdata\\roaming\\python\\python310\\site-packages (from pyttsx3) (304)\n",
      "Collecting pypiwin32\n",
      "  Downloading pypiwin32-223-py3-none-any.whl (1.7 kB)\n",
      "Collecting comtypes\n",
      "  Downloading comtypes-1.2.0-py2.py3-none-any.whl (184 kB)\n",
      "     -------------------------------------- 184.3/184.3 kB 5.4 MB/s eta 0:00:00\n",
      "Installing collected packages: comtypes, pypiwin32, pyttsx3\n",
      "Successfully installed comtypes-1.2.0 pypiwin32-223 pyttsx3-2.90\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyttsx3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e760155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "\n",
    "# Initialize GPT-J-6B\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Initialize speech recognition\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "def get_user_input():\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening...\")\n",
    "        audio_data = recognizer.listen(source)\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "    return text\n",
    "\n",
    "def generate_response(prompt):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length=100, num_return_sequences=1)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "while True:\n",
    "    # Get user input via microphone\n",
    "    user_input = get_user_input()\n",
    "    print(f\"User: {user_input}\")\n",
    "\n",
    "    # Generate a response using GPT-J-6B\n",
    "    prompt = f\"The following is a conversation with an AI assistant. The assistant is helpful, creative, and has access to a large amount of information.\\n\\nUser: {user_input}\\nAssistant:\"\n",
    "    assistant_response = generate_response(prompt)\n",
    "    print(f\"Assistant: {assistant_response}\")\n",
    "\n",
    "    # Speak the response\n",
    "    engine.say(assistant_response)\n",
    "    engine.runAndWait()\n",
    "\n",
    "    # Check if user wants to continue\n",
    "    continue_chat = input(\"Do you want to continue the conversation? (yes/no): \")\n",
    "    if continue_chat.lower() != 'yes':\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a566e11f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
